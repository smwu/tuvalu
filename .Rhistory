K=K, p=p, d=d, S=S)
# Get log-likelihood
# c_i, x1, and x2 are only used to select pi[c_i], theta[1, c_i, x1], and
# theta[2, c_i, x2]. They do not appear in log_lik
log_lik <- paste("~ w * ((log(pi) + log(theta_1) + log(theta_2) +
y*log(pnorm(xi_1*s1 + xi_2*s2)) +
(1-y)*log(1-pnorm(xi_1*s1 + xi_2*s2))))" )
params <- c("pi", "theta_1", "theta_2", "xi_1", "xi_2")
args <- c("w", "y", "s1", "s2", params)
Hess_fn <- deriv(as.formula(log_lik), namevec=params, function.arg=args,
hessian = TRUE, func = TRUE)
# Get Hessian
args_eval <- "w, y, s1, s2, pi, theta_1, theta_2, xi_1, xi_2"
num_params <- (K + (p*K*d) + (S*K))
loglik_all <- numeric(n)
grad_expanded <- matrix(0, nrow=n, ncol=num_params)
Hess_expanded <- array(0, dim=c(n, num_params, num_params))
for (i in 1:n) {
w <- w_all[i]
y <- y_all[i]
x1 <- x1_all[i]
x2 <- x2_all[i]
s1 <- s1_all[i]
s2 <- s2_all[i]
c_i <- c_i_all[i]
pi <- pi_all[c_i]
theta_1 <- theta_1_all[c_i, x1]
theta_2 <- theta_2_all[c_i, x2]
xi_1 <- xi_1_all[c_i]
xi_2 <- xi_2_all[c_i]
get_Hess <- eval(parse(text = paste0("Hess_fn(", args_eval, ")")))
grad <- attr(get_Hess, "gradient")
hess <- attr(get_Hess, "hessian")
loglik_all[i] <- get_Hess
indices <- c(c_i, K+K*(c_i-1)+x1, K+(K*d)+K*(c_i-1)+x2, K+(p*K*d)+K*(c_i-1)+(1:S))
grad_expanded[i, indices] = grad
Hess_expanded[i, indices, indices] = hess
}
loglik_sum <- sum(loglik_all)
grad_sum <- colSums(grad_expanded)
Hess_sum <- colSums(Hess_expanded, dims = 1)
asymp_var <- solve(-Hess_sum)
### Compare auto-diff with analytical form
grad_sum - grad_analytic_sum
Hess_sum - Hess_analytic_sum
asymp_var - asymp_var_analytic
# Create survey design
svy_data <- data.frame(s = s2_all,
x = cbind(x1_all, x2_all),
y = y_all,
wts = w_all)
svydes <- svydesign(ids = ~1, strata = ~s, weights = ~wts, data = svy_data)
# create svrepdesign
svyrep <- as.svrepdesign(design = svydes, type = "mrbbootstrap",
replicates = 100)
# Wrapper function to calculate gradient using different replicate weights
#' @param rep_wts Vector of replicate weights from the svyrep object
#' @param svy_data Data frame with variables from svydes object. Must be the second argument
get_grad_reps <- function(rep_wts, svy_data, log_lik, params, args, args_eval, data_all,
params_all, gsub_str, K, p, d, S) {
data_all$w_all <- rep_wts
grad_hess <- get_grad_hess(log_lik=log_lik, params=params, args=args, args_eval=args_eval,
data_all=data_all, params_all=params_all, gsub_str=gsub_str,
K=K, p=p, d=d, S=S)
grad_reps <- grad_hess$grad_sum
return(grad_reps)
}
rep_temp <- withReplicates(design = svyrep, theta = get_grad_reps,
log_lik=log_lik, params=params, args=args, args_eval=args_eval,
data_all=data_all, params_all=params_all, gsub_str=gsub_str,
K=K, p=p, d=d, S=S)
Jhat <- vcov(rep_temp)
# compute adjustment
Hi <- asymp_var
V1 <- Hi %*% Jhat %*% Hi
V2 <- Hi %*% (Jhat %*% Hi)
identical(V1, V2)
eigen(V1)$values
eigen(V2)$values
eigen(Hi)$values
chol(V1)
V1_pd <- nearPD(V1)
eigen(V1_pd)$values
eigen(V1_pd$mat)$values
eigen(V1)$values
pi_red <- matrix(replicate(100, pi_all), nrow=100, byrow=TRUE)
head(pi_red)
noise <- rnorm(100, mean=0, sd=0.1)
pi_all + rnorm(2, 0, 0.1)
set.seed(1)
pi_red <- matrix(replicate(100, pi_all), nrow=100, byrow=TRUE) + rnorm(200, 0, 0.1)
head(pi_red)
colMeans(pi_red)
theta_1_all
theta_1_red <- theta_2_red <- array(replicate(100, theta_1_all), dim=c(100, 2, 2))
theta_1_red
theta_1_red[1,,]
theta_1_red[2,,]
xi_1_all
theta_1_red <- theta_2_red <- replicate(100, theta_1_all, simplify="array")
theta_1_red[1,,]
theta_1_all
pi_red <- matrix(replicate(100, pi_all), nrow=100, byrow=TRUE) + rnorm(200, 0, 0.1)
theta_1_red <- theta_2_red <- array(NA, dim=c(100, 2, 2))
for (i in 1:100) {
theta_1_red[i,,] <- theta_1_all + rnorm(4, 0, 0.1)
theta_2_red[i,,] <- theta_2_all + rnorm(4, 0, 0.1)
}
theta_1_all[1,,]
theta_1_red[1,,]
theta_1_red <- theta_2_red <- array(NA, dim=c(100, 2, 2))
for (i in 1:100) {
theta_1_red[i,,] <- theta_1_all + rnorm(4, 0, 0.01)
theta_2_red[i,,] <- theta_2_all + rnorm(4, 0, 0.01)
}
theta_1_red[1,,]
theta_1_red[2,,]
pi_all
theta_1_all
xi_1_all
xi_2_all
pi_all <- colMeans(pi_red)
theta_1_all <- colMeans(theta_1_red, dims = 1)
theta_2_all <- colMeans(theta_2_red, dims = 1)
xi_1_all <- colMeans(xi_1_red)
xi_1_red <- matrix(replicate(100, xi_1_all), nrow=100, byrow=TRUE) + rnorm(200, 0, 0.1)
xi_2_red <- matrix(replicate(100, xi_2_all), nrow=100, byrow=TRUE) + rnorm(200, 0, 0.1)
xi_1_all <- colMeans(xi_1_red)
xi_2_all <- colMeans(xi_2_red)
pi_all
theta_1_all
theta_2_all
xi_1_all
xi_2_all
num_params <- (K + (p*K*d) + (S*K))
grad_analytic <- matrix(0, nrow=n, ncol=num_params)
Hess_analytic <- array(0, dim=c(n, num_params, num_params))
for (i in 1:n) {
w <- w_all[i]
y <- y_all[i]
x1 <- x1_all[i]
x2 <- x2_all[i]
s1 <- s1_all[i]
s2 <- s2_all[i]
c_i <- c_i_all[i]
pi <- pi_all[c_i]
theta_1 <- theta_1_all[c_i, x1]
theta_2 <- theta_2_all[c_i, x2]
xi_1 <- xi_1_all[c_i]
xi_2 <- xi_2_all[c_i]
eta <- xi_1*s1+xi_2*s2
indices <- c(c_i, K+K*(c_i-1)+x1, K+(K*d)+K*(c_i-1)+x2, K+(p*K*d)+K*(c_i-1)+(1:S))
grad_analytic[i, indices] <- c(w/pi, w/theta_1, w/theta_2,
w*(y*s1*dnorm(eta)/pnorm(eta) -
((1-y)*s1*dnorm(eta)/(1-pnorm(eta)))),
w*(y*s2*dnorm(eta)/pnorm(eta) -
((1-y)*s2*dnorm(eta)/(1-pnorm(eta)))) )
diags <- c(-w/pi^2, -w/theta_1^2, -w/theta_2^2,
w*(-y*((s1^2)*(eta*dnorm(eta)*pnorm(eta)+(dnorm(eta)^2)) / (pnorm(eta)^2))
+ ((1-y)*(s1^2)*(eta*dnorm(eta)*(1-pnorm(eta))-(dnorm(eta)^2)) / ((1-pnorm(eta))^2))),
w*(-y*(s2^2)*(eta*dnorm(eta)*pnorm(eta)+(dnorm(eta)^2)) / (pnorm(eta)^2)
+ ((1-y)*(s2^2)*(eta*dnorm(eta)*(1-pnorm(eta))-(dnorm(eta)^2)) / ((1-pnorm(eta))^2))) )
Hess_analytic[i, indices, indices] = diag(diags)
}
grad_analytic_sum <- colSums(grad_analytic)
names(grad_analytic_sum) <- c(paste0("pi_", 1:K),
paste0("theta_", rep(1:p, each=(K*d)), rep(1:K, each=d), rep(1:d)),
paste0("xi_", rep(1:K, each=S), rep(1:S)))
Hess_analytic_sum <- colSums(Hess_analytic, dims = 1)
asymp_var_analytic <- solve(-Hess_analytic_sum)
# Get log-likelihood
# c_i, x1, and x2 are only used to select pi[c_i], theta[1, c_i, x1], and
# theta[2, c_i, x2]. They do not appear in log_lik
log_lik <- paste("~ w * ((log(pi) + log(theta_1) + log(theta_2) +
y*log(pnorm(xi_1*s1 + xi_2*s2)) +
(1-y)*log(1-pnorm(xi_1*s1 + xi_2*s2))))" )
params <- c("pi", "theta_1", "theta_2", "xi_1", "xi_2")
args <- c("w", "y", "s1", "s2", params)
Hess_fn <- deriv(as.formula(log_lik), namevec=params, function.arg=args,
hessian = TRUE, func = TRUE)
# Get Hessian
args_eval <- "w, y, s1, s2, pi, theta_1, theta_2, xi_1, xi_2"
num_params <- (K + (p*K*d) + (S*K))
loglik_all <- numeric(n)
grad_expanded <- matrix(0, nrow=n, ncol=num_params)
Hess_expanded <- array(0, dim=c(n, num_params, num_params))
for (i in 1:n) {
w <- w_all[i]
y <- y_all[i]
x1 <- x1_all[i]
x2 <- x2_all[i]
s1 <- s1_all[i]
s2 <- s2_all[i]
c_i <- c_i_all[i]
pi <- pi_all[c_i]
theta_1 <- theta_1_all[c_i, x1]
theta_2 <- theta_2_all[c_i, x2]
xi_1 <- xi_1_all[c_i]
xi_2 <- xi_2_all[c_i]
get_Hess <- eval(parse(text = paste0("Hess_fn(", args_eval, ")")))
grad <- attr(get_Hess, "gradient")
hess <- attr(get_Hess, "hessian")
loglik_all[i] <- get_Hess
indices <- c(c_i, K+K*(c_i-1)+x1, K+(K*d)+K*(c_i-1)+x2, K+(p*K*d)+K*(c_i-1)+(1:S))
grad_expanded[i, indices] = grad
Hess_expanded[i, indices, indices] = hess
}
loglik_sum <- sum(loglik_all)
grad_sum <- colSums(grad_expanded)
Hess_sum <- colSums(Hess_expanded, dims = 1)
asymp_var <- solve(-Hess_sum)
### Compare auto-diff with analytical form
grad_sum - grad_analytic_sum
Hess_sum - Hess_analytic_sum
asymp_var - asymp_var_analytic
asymp_var
diag(asymp_var)
c(paste0("pi", 1:K),
paste0("theta", 1:p, "_", rep(1:K, each=p), "_", rep(1:d, each=p*K)),
paste0("xi", 1:q))
temp <- diag(asymp_var)
names(temp) <- c(paste0("pi", 1:K),
paste0("theta", 1:p, "_", rep(1:K, each=p), "_", rep(1:d, each=p*K)),
paste0("xi", 1:q))
temp
eigen(asymp_var)$values
# Create survey design
svy_data <- data.frame(s = s2_all,
x = cbind(x1_all, x2_all),
y = y_all,
wts = w_all)
svydes <- svydesign(ids = ~1, strata = ~s, weights = ~wts, data = svy_data)
# create svrepdesign
svyrep <- as.svrepdesign(design = svydes, type = "mrbbootstrap",
replicates = 100)
# Wrapper function to calculate gradient using different replicate weights
#' @param rep_wts Vector of replicate weights from the svyrep object
#' @param svy_data Data frame with variables from svydes object. Must be the second argument
get_grad_reps <- function(rep_wts, svy_data, log_lik, params, args, args_eval, data_all,
params_all, gsub_str, K, p, d, S) {
data_all$w_all <- rep_wts
grad_hess <- get_grad_hess(log_lik=log_lik, params=params, args=args, args_eval=args_eval,
data_all=data_all, params_all=params_all, gsub_str=gsub_str,
K=K, p=p, d=d, S=S)
grad_reps <- grad_hess$grad_sum
return(grad_reps)
}
rep_temp <- withReplicates(design = svyrep, theta = get_grad_reps,
log_lik=log_lik, params=params, args=args, args_eval=args_eval,
data_all=data_all, params_all=params_all, gsub_str=gsub_str,
K=K, p=p, d=d, S=S)
Jhat <- vcov(rep_temp)
# compute adjustment
Hi <- asymp_var
V1 <- Hi %*% Jhat %*% Hi
V2 <- Hi %*% (Jhat %*% Hi)
# check issue with p.d. projection
if (min(diag(V1)) < 0) {
print("V1 has negative variances")
}
if (min(diag(Hi)) < 0) {
print("Hi has negative variances")
}
eigen(V1)$values
eigen(Hi)$values
# if matrices are not p.d. due to rounding issues, convert to nearest p.d. matrix
# first, using method proposed in Higham (2002)
if (min(Re(eigen(V1)$values)) < 1e-15) {
V1_pd <- nearPD(V1)
R1 <- chol(V1_pd$mat)
} else {
R1 <- chol(V1)
}
if (min(Re(eigen(Hi)$values)) < 1e-15) {
Hi_pd <- nearPD(Hi)
R2i <- chol(Hi_pd$mat)
} else {
R2i <- chol(Hi)
}
R2 <- solve(R2i)
R2R1 <- R2 %*% R1
#' Helper function to apply matrix rotation
#' @param par unadjusted parameter estimates
#' @param par_hat unadjusted mean parameter estimates
#' @param R2R1 adjustment matrix
#' @return adjusted parameter estimates
DEadj <- function(par, par_hat, R2R1) {
par_adj <- (par - par_hat) %*% R2R1 + par_hat
par_adj <- as.vector(par_adj)
return(par_adj)
}
pi_adj <- DEadj(pi_red, pi_all, R2R1)
R2R1
par_samps <- cbind(pi_red, theta_1_red, theta_2_red, xi_1_red, xi_2_red)
c(theta_1_all)
names(temp) <- c(paste0("pi_", 1:K),
paste0("theta_", rep(1:p, each=(K*d)), rep(1:K, each=d), rep(1:d)),
paste0("xi_", rep(1:K, each=S), rep(1:S)))
temp
dim(theta_1_red)
theta_1_vect <- theta_1_red
dim(theta_1_vect) <- c(M, K*d)
M <- 100
dim(theta_1_vect) <- c(M, K*d)
head(theta_1_vect)
theta_1_all
colMeans(theta_1_vect)
temp2 <- t(theta_1_red)
head(theta_1_red)
temp2 <- as.matrix(theta_1_vect, nrow=M, ncol=K*d, byrow=FALSE)
head(temp2)
paste0("theta", 1:p, "_", rep(1:K, each=p), "_", rep(1:d, each=p*K))
dim(pi_red)
# theta_1_vect <- theta_1_red
# dim(theta_1_vect) <- c(M, K*d)
par_samps <- cbind(pi_red,
theta_1_red[,1,1], theta_1_red[,1,2], theta_1_red[,2,1], theta_1_red[,2,2],
theta_2_red[,1,1], theta_2_red[,1,2], theta_2_red[,2,1], theta_2_red[,2,2],
xi_1_red, xi_2_red)
colnames(par_samps) <- c(paste0("pi_", 1:K),
paste0("theta_", rep(1:p, each=(K*d)), rep(1:K, each=d), rep(1:d)),
paste0("xi_", rep(1:K, each=S), rep(1:S)))
par_hat <- colMeans(par_samps)
par_hat
pi_all
theta_1_all
xi_1_all
par_hat <- colMeans(par_samps)
par_adj <- apply(par_samps, 1, DEadj, par_hat = par_hat, R2R1 = R2R1, simplify = FALSE)
colMeans(par_adj)
colMeans(par_adj, dim=1)
par_adj <- matrix(unlist(par_adj), byrow=TRUE, nrow=M)
par_means_auto <- colMeans(par_adj)
par_vars_auto <- apply(par_adj, 1, var)
par_means_auto
par_vars_auto
par_vars_auto <- apply(par_adj, 2, var)
par_vars_auto
par_means_auto
abind(theta_1_red, theta_2_red, along=2)
temp <- abind(theta_1_red, theta_2_red, along=2)
dim(temp) <- c(M, p, K, d)
temp[1,1,,]
temp <- abind(theta_1_red, theta_2_red, along=3)
temp <- abind(theta_1_red, theta_2_red, along=4)
temp <- abind(theta_1_red, theta_2_red, along=1)
dim(temp) <- c(M, p, K, d)
temp[1,1,,]
theta_1_red[1,,]
analysis$xi_med
xi_1_all
analysis <- list()
analysis$c_i <- c_i_all
analysis$pi_med <- pi_all
analysis$theta_med <- array(c(0.9, 0.9, 0.1, 0.1, 0.1, 0.1, 0.9, 0.9), dim=c(p,K,d))
analysis$xi_med <- c(qnorm(0.5), qnorm(2/3), qnorm(0.5), qnorm(1/3))
s <- s2_all + 1
X <- cbind(x1_all, x2_all)
# Get median posterior parameter estimates
par_hat <- c(analysis$pi_med, c(analysis$theta_med), analysis$xi_med)
names(par_hat) <- c(paste0("pi", 1:K),
paste0("theta", 1:p, "_", rep(1:K, each=p), "_", rep(1:d, each=p*K)),
paste0("xi", 1:q))
# Create probit design matrix from dummy variables
dummy_s <- dummy_cols(data.frame(s = factor(s)),
remove_selected_columns = TRUE)
dummy_c <- dummy_cols(data.frame(c = factor(analysis$c_i, levels=1:K)),
remove_selected_columns = TRUE)
V <- as.data.frame(apply(dummy_s, 2, function(col) t(col) * dummy_c))
# Create array with assigned classes
V_k <- array(NA, dim=c(K, n, q))
for (k in 1:K) {
temp_data <- data.frame(s = factor(s, levels=1:S),
c = factor(rep(k, n), levels=1:K))
V_k[k, ,] <- model.matrix(~ c * s, data = temp_data)
}
data_stan <- list(K = K, p = p, d = d, n = n, q = q, X = X, y = y_all, V_k = V_k,
weights = w_all, alpha = alpha, eta = eta, mu0 = mu0, Sig0 = Sig0)
# Create Stan model
mod_stan <- stan_model(paste0(analysis_dir, "mixture_model.stan"))
# Stan parameters of interest
par_stan <- c('pi', 'theta', 'xi')  # subset of parameters interested in
# Run Stan model
# Stan will pass warnings from calling 0 chains, but will still create an
# out_stan object for the 'grad_log_prob()' method
out_stan <- sampling(object = mod_stan, data = data_stan, pars = par_stan,
chains = 0, iter = 0, refresh = 0)
q <- 4
n_chains <- 1
alpha <- rep(1, K)/K
eta <- matrix(1, nrow=K, ncol=d)
mu0 <- rep(0, q)
Sig0 <- diag(rep(1, q), nrow=q, ncol=q)
analysis <- list()
analysis$c_i <- c_i_all
analysis$pi_med <- pi_all
analysis$theta_med <- array(c(0.9, 0.9, 0.1, 0.1, 0.1, 0.1, 0.9, 0.9), dim=c(p,K,d))
analysis$xi_med <- c(qnorm(0.5), qnorm(2/3), qnorm(0.5), qnorm(1/3))
s <- s2_all + 1
X <- cbind(x1_all, x2_all)
# Get median posterior parameter estimates
par_hat <- c(analysis$pi_med, c(analysis$theta_med), analysis$xi_med)
names(par_hat) <- c(paste0("pi", 1:K),
paste0("theta", 1:p, "_", rep(1:K, each=p), "_", rep(1:d, each=p*K)),
paste0("xi", 1:q))
# Create probit design matrix from dummy variables
dummy_s <- dummy_cols(data.frame(s = factor(s)),
remove_selected_columns = TRUE)
dummy_c <- dummy_cols(data.frame(c = factor(analysis$c_i, levels=1:K)),
remove_selected_columns = TRUE)
V <- as.data.frame(apply(dummy_s, 2, function(col) t(col) * dummy_c))
# Create array with assigned classes
V_k <- array(NA, dim=c(K, n, q))
for (k in 1:K) {
temp_data <- data.frame(s = factor(s, levels=1:S),
c = factor(rep(k, n), levels=1:K))
V_k[k, ,] <- model.matrix(~ c * s, data = temp_data)
}
data_stan <- list(K = K, p = p, d = d, n = n, q = q, X = X, y = y_all, V_k = V_k,
weights = w_all, alpha = alpha, eta = eta, mu0 = mu0, Sig0 = Sig0)
# Create Stan model
mod_stan <- stan_model(paste0(analysis_dir, "mixture_model.stan"))
# Stan parameters of interest
par_stan <- c('pi', 'theta', 'xi')  # subset of parameters interested in
# Run Stan model
# Stan will pass warnings from calling 0 chains, but will still create an
# out_stan object for the 'grad_log_prob()' method
out_stan <- sampling(object = mod_stan, data = data_stan, pars = par_stan,
chains = 0, iter = 0, refresh = 0)
# get dimension of unconstrained parameter space
get_num_upars(out_stan)  #278 unconstrained, 369 constrained
# convert params from constrained space to unconstrained space
unc_par_hat <- unconstrain_pars(out_stan, list("pi" = c(analysis$pi_med),
"theta" = analysis$theta_med,
"xi" = c(analysis$xi_med)))
analysis$pi_med <- c(0.4, 0.6)
analysis$theta_med <- array(c(0.9, 0.9, 0.1, 0.1, 0.1, 0.1, 0.9, 0.9), dim=c(p,K,d))
analysis$xi_med <- c(qnorm(0.5), qnorm(2/3), qnorm(0.5), qnorm(1/3))
# Get median posterior parameter estimates
par_hat <- c(analysis$pi_med, c(analysis$theta_med), analysis$xi_med)
names(par_hat) <- c(paste0("pi", 1:K),
paste0("theta", 1:p, "_", rep(1:K, each=p), "_", rep(1:d, each=p*K)),
paste0("xi", 1:q))
data_stan <- list(K = K, p = p, d = d, n = n, q = q, X = X, y = y_all, V_k = V_k,
weights = w_all, alpha = alpha, eta = eta, mu0 = mu0, Sig0 = Sig0)
# Create Stan model
mod_stan <- stan_model(paste0(analysis_dir, "mixture_model.stan"))
# Stan parameters of interest
par_stan <- c('pi', 'theta', 'xi')  # subset of parameters interested in
# Run Stan model
# Stan will pass warnings from calling 0 chains, but will still create an
# out_stan object for the 'grad_log_prob()' method
out_stan <- sampling(object = mod_stan, data = data_stan, pars = par_stan,
chains = 0, iter = 0, refresh = 0)
# get dimension of unconstrained parameter space
get_num_upars(out_stan)  #278 unconstrained, 369 constrained
# convert params from constrained space to unconstrained space
unc_par_hat <- unconstrain_pars(out_stan, list("pi" = c(analysis$pi_med),
"theta" = analysis$theta_med,
"xi" = c(analysis$xi_med)))
xi_1_all
xi_2_all
analysis$xi_med
xi_red <- cbind(xi_1_red[,1], xi_2_red[,1], xi_1_red[,2], xi_2_red[,2])
unc_par_samps <- lapply(1:M, unconstrain, stan_model = out_stan,
pi = pi_red, theta = theta_red, xi = xi_red)
unc_par_samps <- lapply(1:M, unconstrain_pars, stan_model = out_stan,
pi = pi_red, theta = theta_red, xi = xi_red)
# 'unconstrain' converts from constrained space to unconstrained space for one
# row, given input arrays of MCMC parameter output
# Inputs:
#   i: row index
#   stan_model: stan model
#   pi: MCMC matrix output for pi; M rows
#   theta: MCMC array output for theta; dim 1 length = M
#   xi: MCMC matrix output for xi: M rows
# Output: vector of unconstrained parameters
unconstrain <- function(i, stan_model, pi, theta, xi) {
upars <- unconstrain_pars(stan_model, list("pi" = pi[i,],
"theta" = theta[i,,,], "xi" = xi[i,]))
return(upars)
}
unc_par_samps <- lapply(1:M, unconstrain, stan_model = out_stan,
pi = pi_red, theta = theta_red, xi = xi_red)
# Unconstrained parameters for all MCMC samples
theta_red <- abind(theta_1_red, theta_2_red, along=1)
dim(theta_red) <- c(M, p, K, d)
xi_red <- cbind(xi_1_red[,1], xi_2_red[,1], xi_1_red[,2], xi_2_red[,2])
unc_par_samps <- lapply(1:M, unconstrain, stan_model = out_stan,
pi = pi_red, theta = theta_red, xi = xi_red)
par_means_auto
par_vars_auto
par_samps
par_hat
par_means_auto
# theta_1_vect <- theta_1_red
# dim(theta_1_vect) <- c(M, K*d)
par_samps <- cbind(pi_red,
theta_1_red[,1,1], theta_1_red[,1,2], theta_1_red[,2,1], theta_1_red[,2,2],
theta_2_red[,1,1], theta_2_red[,1,2], theta_2_red[,2,1], theta_2_red[,2,2],
xi_1_red, xi_2_red)
colnames(par_samps) <- c(paste0("pi_", 1:K),
paste0("theta_", rep(1:p, each=(K*d)), rep(1:K, each=d), rep(1:d)),
paste0("xi_", rep(1:K, each=S), rep(1:S)))
par_hat <- colMeans(par_samps)
par_adj <- apply(par_samps, 1, DEadj, par_hat = par_hat, R2R1 = R2R1, simplify = FALSE)
par_adj <- matrix(unlist(par_adj), byrow=TRUE, nrow=M)
par_means_auto <- colMeans(par_adj)
par_vars_auto <- apply(par_adj, 2, var)
par_means_auto
par_hat
par_means_auto - par_hat
