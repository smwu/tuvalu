s <- s2_all + 1
X <- cbind(x1_all, x2_all)
# Get median posterior parameter estimates
par_hat <- c(analysis$pi_med, c(analysis$theta_med), analysis$xi_med)
names(par_hat) <- c(paste0("pi", 1:K),
paste0("theta", 1:p, "_", rep(1:K, each=p), "_", rep(1:d, each=p*K)),
paste0("xi", 1:q))
# Create probit design matrix from dummy variables
dummy_s <- dummy_cols(data.frame(s = factor(s)),
remove_selected_columns = TRUE)
dummy_c <- dummy_cols(data.frame(c = factor(analysis$c_i, levels=1:K)),
remove_selected_columns = TRUE)
V <- as.data.frame(apply(dummy_s, 2, function(col) t(col) * dummy_c))
# Create array with assigned classes
V_k <- array(NA, dim=c(K, n, q))
for (k in 1:K) {
temp_data <- data.frame(s = factor(s, levels=1:S),
c = factor(rep(k, n), levels=1:K))
V_k[k, ,] <- model.matrix(~ c * s, data = temp_data)
}
data_stan <- list(K = K, p = p, d = d, n = n, q = q, X = X, y = y_all, V_k = V_k,
weights = w_all, alpha = alpha, eta = eta, mu0 = mu0, Sig0 = Sig0)
# Create Stan model
mod_stan <- stan_model(paste0(analysis_dir, "mixture_model.stan"))
# Stan parameters of interest
par_stan <- c('pi', 'theta', 'xi')  # subset of parameters interested in
# Run Stan model
# Stan will pass warnings from calling 0 chains, but will still create an
# out_stan object for the 'grad_log_prob()' method
out_stan <- sampling(object = mod_stan, data = data_stan, pars = par_stan,
chains = 0, iter = 0, refresh = 0)
q <- 4
n_chains <- 1
alpha <- rep(1, K)/K
eta <- matrix(1, nrow=K, ncol=d)
mu0 <- rep(0, q)
Sig0 <- diag(rep(1, q), nrow=q, ncol=q)
analysis <- list()
analysis$c_i <- c_i_all
analysis$pi_med <- pi_all
analysis$theta_med <- array(c(0.9, 0.9, 0.1, 0.1, 0.1, 0.1, 0.9, 0.9), dim=c(p,K,d))
analysis$xi_med <- c(qnorm(0.5), qnorm(2/3), qnorm(0.5), qnorm(1/3))
s <- s2_all + 1
X <- cbind(x1_all, x2_all)
# Get median posterior parameter estimates
par_hat <- c(analysis$pi_med, c(analysis$theta_med), analysis$xi_med)
names(par_hat) <- c(paste0("pi", 1:K),
paste0("theta", 1:p, "_", rep(1:K, each=p), "_", rep(1:d, each=p*K)),
paste0("xi", 1:q))
# Create probit design matrix from dummy variables
dummy_s <- dummy_cols(data.frame(s = factor(s)),
remove_selected_columns = TRUE)
dummy_c <- dummy_cols(data.frame(c = factor(analysis$c_i, levels=1:K)),
remove_selected_columns = TRUE)
V <- as.data.frame(apply(dummy_s, 2, function(col) t(col) * dummy_c))
# Create array with assigned classes
V_k <- array(NA, dim=c(K, n, q))
for (k in 1:K) {
temp_data <- data.frame(s = factor(s, levels=1:S),
c = factor(rep(k, n), levels=1:K))
V_k[k, ,] <- model.matrix(~ c * s, data = temp_data)
}
data_stan <- list(K = K, p = p, d = d, n = n, q = q, X = X, y = y_all, V_k = V_k,
weights = w_all, alpha = alpha, eta = eta, mu0 = mu0, Sig0 = Sig0)
# Create Stan model
mod_stan <- stan_model(paste0(analysis_dir, "mixture_model.stan"))
# Stan parameters of interest
par_stan <- c('pi', 'theta', 'xi')  # subset of parameters interested in
# Run Stan model
# Stan will pass warnings from calling 0 chains, but will still create an
# out_stan object for the 'grad_log_prob()' method
out_stan <- sampling(object = mod_stan, data = data_stan, pars = par_stan,
chains = 0, iter = 0, refresh = 0)
# get dimension of unconstrained parameter space
get_num_upars(out_stan)  #278 unconstrained, 369 constrained
# convert params from constrained space to unconstrained space
unc_par_hat <- unconstrain_pars(out_stan, list("pi" = c(analysis$pi_med),
"theta" = analysis$theta_med,
"xi" = c(analysis$xi_med)))
analysis$pi_med <- c(0.4, 0.6)
analysis$theta_med <- array(c(0.9, 0.9, 0.1, 0.1, 0.1, 0.1, 0.9, 0.9), dim=c(p,K,d))
analysis$xi_med <- c(qnorm(0.5), qnorm(2/3), qnorm(0.5), qnorm(1/3))
# Get median posterior parameter estimates
par_hat <- c(analysis$pi_med, c(analysis$theta_med), analysis$xi_med)
names(par_hat) <- c(paste0("pi", 1:K),
paste0("theta", 1:p, "_", rep(1:K, each=p), "_", rep(1:d, each=p*K)),
paste0("xi", 1:q))
data_stan <- list(K = K, p = p, d = d, n = n, q = q, X = X, y = y_all, V_k = V_k,
weights = w_all, alpha = alpha, eta = eta, mu0 = mu0, Sig0 = Sig0)
# Create Stan model
mod_stan <- stan_model(paste0(analysis_dir, "mixture_model.stan"))
# Stan parameters of interest
par_stan <- c('pi', 'theta', 'xi')  # subset of parameters interested in
# Run Stan model
# Stan will pass warnings from calling 0 chains, but will still create an
# out_stan object for the 'grad_log_prob()' method
out_stan <- sampling(object = mod_stan, data = data_stan, pars = par_stan,
chains = 0, iter = 0, refresh = 0)
# get dimension of unconstrained parameter space
get_num_upars(out_stan)  #278 unconstrained, 369 constrained
# convert params from constrained space to unconstrained space
unc_par_hat <- unconstrain_pars(out_stan, list("pi" = c(analysis$pi_med),
"theta" = analysis$theta_med,
"xi" = c(analysis$xi_med)))
xi_1_all
xi_2_all
analysis$xi_med
xi_red <- cbind(xi_1_red[,1], xi_2_red[,1], xi_1_red[,2], xi_2_red[,2])
unc_par_samps <- lapply(1:M, unconstrain, stan_model = out_stan,
pi = pi_red, theta = theta_red, xi = xi_red)
unc_par_samps <- lapply(1:M, unconstrain_pars, stan_model = out_stan,
pi = pi_red, theta = theta_red, xi = xi_red)
# 'unconstrain' converts from constrained space to unconstrained space for one
# row, given input arrays of MCMC parameter output
# Inputs:
#   i: row index
#   stan_model: stan model
#   pi: MCMC matrix output for pi; M rows
#   theta: MCMC array output for theta; dim 1 length = M
#   xi: MCMC matrix output for xi: M rows
# Output: vector of unconstrained parameters
unconstrain <- function(i, stan_model, pi, theta, xi) {
upars <- unconstrain_pars(stan_model, list("pi" = pi[i,],
"theta" = theta[i,,,], "xi" = xi[i,]))
return(upars)
}
unc_par_samps <- lapply(1:M, unconstrain, stan_model = out_stan,
pi = pi_red, theta = theta_red, xi = xi_red)
# Unconstrained parameters for all MCMC samples
theta_red <- abind(theta_1_red, theta_2_red, along=1)
dim(theta_red) <- c(M, p, K, d)
xi_red <- cbind(xi_1_red[,1], xi_2_red[,1], xi_1_red[,2], xi_2_red[,2])
unc_par_samps <- lapply(1:M, unconstrain, stan_model = out_stan,
pi = pi_red, theta = theta_red, xi = xi_red)
par_means_auto
par_vars_auto
par_samps
par_hat
par_means_auto
# theta_1_vect <- theta_1_red
# dim(theta_1_vect) <- c(M, K*d)
par_samps <- cbind(pi_red,
theta_1_red[,1,1], theta_1_red[,1,2], theta_1_red[,2,1], theta_1_red[,2,2],
theta_2_red[,1,1], theta_2_red[,1,2], theta_2_red[,2,1], theta_2_red[,2,2],
xi_1_red, xi_2_red)
colnames(par_samps) <- c(paste0("pi_", 1:K),
paste0("theta_", rep(1:p, each=(K*d)), rep(1:K, each=d), rep(1:d)),
paste0("xi_", rep(1:K, each=S), rep(1:S)))
par_hat <- colMeans(par_samps)
par_adj <- apply(par_samps, 1, DEadj, par_hat = par_hat, R2R1 = R2R1, simplify = FALSE)
par_adj <- matrix(unlist(par_adj), byrow=TRUE, nrow=M)
par_means_auto <- colMeans(par_adj)
par_vars_auto <- apply(par_adj, 2, var)
par_means_auto
par_hat
par_means_auto - par_hat
library(tidyverse)
library(readxl)
library(lubridate)
library(tableone)
#Output file to excel
library(openxlsx)
#Make plot
library(corrplot)
library(poLCA)
library(insight)
library(forcats)
library(lme4)
library(rstanarm)
options(mc.cores = 4)
#==================== Helper functions ================================
#Create functions to extract estimate, CI and p-values
specify_decimal <- function(x, k) trimws(format(round(x, k), nsmall=k))
linear_coef<-function(b){
cbind(names(b$coefficients[,1]),paste(specify_decimal(b$coefficients[,1],2)," (",specify_decimal(b$coefficients[,1]+qnorm(0.025)*b$coefficients[,2],2),"-",specify_decimal(b$coefficients[,1]+qnorm(c(.975))*b$coefficients[,2],2),", p=",specify_decimal(b$coefficients[,4],3),")",sep=""))
}
logistic_coef<-function(b){
cbind(names(b$coefficients[,1]),paste(specify_decimal(exp(b$coefficients[,1]),2)," (",specify_decimal(exp(b$coefficients[,1]+qnorm(0.025)*b$coefficients[,2]),2),"-",specify_decimal(exp(b$coefficients[,1]+qnorm(c(.975))*b$coefficients[,2]),2),", p=",specify_decimal(b$coefficients[,4],3),")",sep=""))
}
### CHECK MORBID OBESITY CODING
# ============ Fit using Bayesian hierarchical modeling ========================
get_output <- function(fit, exponentiate = TRUE) {
if (exponentiate) {
output <- data.frame(exp(fit$coefficients),
exp(posterior_interval(fit, prob = 0.95)[1:length(fit$coefficients),]))
colnames(output) <- c("Cond'l OR", "2.5%", "97.5%")
} else {
output <- data.frame(fit$coefficients,
posterior_interval(fit, prob = 0.95)[1:length(fit$coefficients),])
colnames(output) <- c("Mean", "2.5%", "97.5%")
}
print(paste0("Number of observations: ", nobs(fit)))
print(output)
}
#import the file, we read the second sheet, which was transposed successfully by the team in Tuvalu (,2) for second sheet
#Note: I use google drive here, so please feel free to set another folder according to where you store the data
#tuvalu<-read_excel("G:/My Drive/tuvalu/Dataset/2022-Nutrition-Survey_20220530.xlsx")
#Set up working directory, save the output into T02 folders
#setwd("G:/My Drive/tuvalu/Results/work/T02/20221208")
setwd("/Users/Stephanie/Documents/GitHub/tuvalu")
tuvalu <- read_excel("2022-Nutrition-Survey_20220530.xlsx")
#Data cleaning 2022 data
#Indicator for those with same responses in knowledge questions
tuvalu$kledg_exclude<-ifelse(apply(tuvalu[, 19:28], 1, function(i) length(unique(i)) > 1)==TRUE,0,1)
tuvalu1<-tuvalu%>%mutate(
#Create year identifier
year="2022",
#Create combine indicator "yearid"= year+study ID
yearid=paste(year,`Participant No.`),
#studyid as categorical one, each subject should have unique ID
studyid=factor(`Participant No.`),
#Study region and date of interview
region=factor(`Interview Region`),
#Main island as 0 (both students and high schoolers), 1 as other islands
region_c=factor(ifelse(region%in%c(1,10),0,1)),
#Grouping of outlying islands, (0) as funafuti, (1) as North three islands (including Nanumea, Nanumaga, and Niutao), (2) as Middle three islands (including Vaitupu, Nukufetau, and Nui), and (3) as South two islands (including Nukulaelae and Niulakita)
region_4group_cat=ifelse(region%in% c(1,10),0,ifelse(region%in% c(2,3,4),1,ifelse(region%in% c(5,6,7,11),2,ifelse(region  %in% c(8,9),3,NA)))),
#Population density of each island
popln_density= ifelse(region %in% c(1,10) , 6320/2.4,
ifelse(region == 10 , 6320/2.4,
ifelse(region == 2 , 512/3.87,
ifelse(region == 3 , 491/3,
ifelse(region == 4 , 582/2.53,
ifelse(region == 6 , 610/2.83,
ifelse(region == 7 , 597/2.99,
ifelse(region == 8 , 300/1.82,
ifelse(region == 9 , 34/0.4,
ifelse(region %in% c(5,11) , 1061/5.6, NA
)))))))))),
# High school student: 0 as Funafuti (interview region 10), 1 as Motufoua High School (interview region 11), 2 as adults
highschool= case_when(
region == 10 ~ 0,
region == 11 ~ 1,
TRUE ~ 2
),
#Adult (1) versus adolescent (2)
adult_c=ifelse(highschool==2,1,0),
#Date of interview
interview_date= ymd(`Interview Date`),
#body measurements
#Height (cm): Exclude extreme values (>250 or <80 cm)
ht=as.numeric(`Height (cm)`),
#Weight: Exclude extreme values (<0 kg)
wt=as.numeric(`Weight (kg)`),
bmi=10000*wt/(ht)^2,
bmi=ifelse(bmi>100,NA,bmi),
#Categorize for overweight and obesity with conventional cutpoint (<25 as 1, 25-30 as 2, and >30 as 3)
bmi_c= cut(bmi, c(0, 25, 30 ,35 ,40 , Inf),
include.lowest = TRUE),
obesity_1=ifelse(bmi>=30,1,0),
obesity_2=ifelse(bmi>=35,1,0),
obesity_3=ifelse(bmi>=40,1,0),
#Waist circumference: Exclude extreme values (<0 cm)
wc=as.numeric(`Waist Circumference (cm)`),
#demographics
#Gender of subjects: Male=1, Female=2
gender=factor(Q1),
#Use gender and wc to define higher waist circ. (Men: >90, female: >80)
wc_h= case_when(
gender==1 & wc>90 ~ 1,
gender==2 & wc>80 ~ 1,
TRUE ~ 0),
#age in years
age=ifelse((2022-as.numeric(Q2))>150,NA,2022-as.numeric(Q2)),
age_c=cut(age, c(0,30,40,50,60,70,Inf)),
#Marital status
marital=factor(Q3),
#Highest education level and construct education level, dichotomized into lower=0 (level 1,2,3; to secondary school) higher=1 (tertiary)
education=factor(Q4),
education_c=factor(ifelse(Q4==4,1,0)),
#Tuvalu citizenship (if==2 then end interview) and residence before 18 y/o
citizen=factor(Q5),
region_before18=factor(Q6),
#How long do we live in current region
region_now_c=factor(Q7),
#1. Regular or fixed work;2. Temporary work; 3. No or students
employ=factor(Q8),
#construct income, dichotomized into lower=0 (level 1,2) and higher=1 (others); considering annual income at 1000 USD, make high versus low at 1000 AUD
income=as.numeric(Q9),
income_c=ifelse(income>1000,1,0),
#Cut by median because only 17 subjects>1000 AUD
income_h=factor(ifelse(income>median(income, na.rm=TRUE),1,0)),
#Self-reported medical history
#hypertension
htn=factor(Q10),
#dyslipidemia
dl=factor(Q11),
#Diabetes
dm=factor(Q12),
# NCD defined as HTN or DM or DL
ncd=factor(ifelse(Q10==1|Q11==1|Q12==1,1,0)),
#Health-related knowledge, change the response into categorical (rename as "kledg1_n")
#For 2022
#The correct answers are 1122211221, so we change the values "kledg_" to correct (1) or not (0, included the unknowns)
kledg1_1=ifelse(Q13==1,1,ifelse(kledg_exclude==1,NA,0)),
kledg1_2=ifelse(Q14==1,1,ifelse(kledg_exclude==1,NA,0)),
kledg1_3=ifelse(Q15==2,1,ifelse(kledg_exclude==1,NA,0)),
kledg1_4=ifelse(Q16==2,1,ifelse(kledg_exclude==1,NA,0)),
kledg1_5=ifelse(Q17==2,1,ifelse(kledg_exclude==1,NA,0)),
kledg1_6=ifelse(Q18==1,1,ifelse(kledg_exclude==1,NA,0)),
kledg1_7=ifelse(Q19==1,1,ifelse(kledg_exclude==1,NA,0)),
kledg1_8=ifelse(Q20==2,1,ifelse(kledg_exclude==1,NA,0)),
kledg1_9=ifelse(Q21==2,1,ifelse(kledg_exclude==1,NA,0)),
kledg1_10=ifelse(Q22==1,1,ifelse(kledg_exclude==1,NA,0)),
kledg_sum=ifelse(kledg_exclude==1,NA,kledg1_1+kledg1_2+kledg1_3+kledg1_4+kledg1_5+kledg1_6+kledg1_7+kledg1_8+kledg1_9+kledg1_10),
kledg_h=ifelse(kledg_sum>median(kledg_sum,na.rm = TRUE),1,ifelse(kledg_exclude==1,NA,0)),
#Health-related attitude, change the response into categorical variable (rename as "att_n"),
#Higher awareness in attitute as 1 (Strongly or agree), otherwise=0.
att_1=as.integer(Q23),
att_2=as.integer(Q24),
att_3=as.integer(Q25),
att_4=as.integer(Q26),
att_5=as.integer(Q27),
att_6=as.integer(Q28),
att_7=as.integer(Q29),
att_8=as.integer(Q30),
att_9=as.integer(Q31),
#Food frequency items
#Keep names as they are (original dataset 38:62, 25 items) except for several one with special chars
Rice=ifelse(Rice<3,1,ifelse(Rice==3,2,ifelse(Rice>3,3,NA))),
`Swamp taro or taro`=ifelse(`Swamp taro or taro`<3,1,ifelse(`Swamp taro or taro`==3,2,ifelse(`Swamp taro or taro`>3,3,NA))),
Breadfruit=ifelse(Breadfruit<3,1,ifelse(Breadfruit==3,2,ifelse(Breadfruit>3,3,NA))),
Fish=ifelse(Fish<3,1,ifelse(Fish==3,2,ifelse(Fish>3,3,NA))),
Pork=ifelse(Pork<3,1,ifelse(Pork==3,2,ifelse(Pork>3,3,NA))),
Cabbage=ifelse(Cabbage<3,1,ifelse(Cabbage==3,2,ifelse(Cabbage>3,3,NA))),
bird_nest_fern=ifelse(bird_nest_fern<3,1,ifelse(bird_nest_fern==3,2,ifelse(bird_nest_fern>3,3,NA))),
Banana=ifelse(Banana<3,1,ifelse(Banana==3,2,ifelse(Banana>3,3,NA))),
Coconut=ifelse(Coconut<3,1,ifelse(Coconut==3,2,ifelse(Coconut>3,3,NA))),
`Imported fruits (Ex. apples, oranges, pears)`=ifelse(`Imported fruits (Ex. apples, oranges, pears)`<3,1,ifelse(`Imported fruits (Ex. apples, oranges, pears)`==3,2,ifelse(`Imported fruits (Ex. apples, oranges, pears)`>3,3,NA))),
Eggs=ifelse(Eggs<3,1,ifelse(Eggs==3,2,ifelse(Eggs>3,3,NA))),
`Sweetened beverages (Ex. coke, juice)`=ifelse(`Sweetened beverages (Ex. coke, juice)`<3,1,ifelse(`Sweetened beverages (Ex. coke, juice)`==3,2,ifelse(`Sweetened beverages (Ex. coke, juice)`>3,3,NA))),
`Ice cream`=ifelse(`Ice cream`<3,1,ifelse(`Ice cream`==3,2,ifelse(`Ice cream`>3,3,NA))),
Potatoes=ifelse(Potatoes<3,1,ifelse(Potatoes==3,2,ifelse(Potatoes>3,3,NA))),
Cassava=ifelse(Cassava<3,1,ifelse(Cassava==3,2,ifelse(Cassava>3,3,NA))),
`Instant noodles`=ifelse(`Instant noodles`<3,1,ifelse(`Instant noodles`==3,2,ifelse(`Instant noodles`>3,3,NA))),
Chicken=ifelse(Chicken<3,1,ifelse(Chicken==3,2,ifelse(Chicken>3,3,NA))),
`Lamb or beef`=ifelse(`Lamb or beef`<3,1,ifelse(`Lamb or beef`==3,2,ifelse(`Lamb or beef`>3,3,NA))),
Cucumber=ifelse(Cucumber<3,1,ifelse(Cucumber==3,2,ifelse(Cucumber>3,3,NA))),
`Imported vegetables`=ifelse(`Imported vegetables`<3,1,ifelse(`Imported vegetables`==3,2,ifelse(`Imported vegetables`>3,3,NA))),
Papaya=ifelse(Papaya<3,1,ifelse(Papaya==3,2,ifelse(Papaya>3,3,NA))),
Pandanus=ifelse(Pandanus<3,1,ifelse(Pandanus==3,2,ifelse(Pandanus>3,3,NA))),
Milk=ifelse(Milk<3,1,ifelse(Milk==3,2,ifelse(Milk>3,3,NA))),
`Chips or biscuits`=ifelse(`Chips or biscuits`<3,1,ifelse(`Chips or biscuits`==3,2,ifelse(`Chips or biscuits`>3,3,NA))),
Cake=ifelse(Cake<3,1,ifelse(Cake==3,2,ifelse(Cake>3,3,NA))),
#import versus domestic: whether interviewees eat more imported staple food, _staple (rice, instant noodle
# versus swamp taro, breadfruit), meat, _meat (pork, lamb/beef, chicken  versus fish); reference as domestic
#Define as imported dominant if they use the import more (2), balanced (1), domestic dominant (0)
# More is defined as using one (or more) of the imported/domestic food item everyday
# Define a binary variable for imported ==1, not==0
food_imp_dom_staple=ifelse(`Instant noodles`==1|Rice==1,2,ifelse(`Swamp taro or taro`==1|Breadfruit==1,0,1)),
food_imported_staple_c=factor(ifelse(food_imp_dom_staple==2,1,0)),
food_imp_dom_meat=ifelse(Pork==1|Chicken==1|`Lamb or beef`==1,2,ifelse(Fish==1,0,1)),
food_imported_meat_c=factor(ifelse(food_imp_dom_meat==2,1,0)),
food_imported_veg_c=factor(ifelse(`Imported vegetables`<=2,1,0)),
food_imported_fruit_c=factor(ifelse(`Imported fruits (Ex. apples, oranges, pears)`<=2,1,0)),
food_imported_sweet_beverage_c=factor(ifelse(`Sweetened beverages (Ex. coke, juice)`<=2,1,0)),
food_imported_ice_cream_c=factor(ifelse(`Ice cream`<=2,1,0)),
food_imported_chip_biscuit_c=factor(ifelse(`Chips or biscuits`<=2,1,0)),
#Diet change (https://stackoverflow.com/questions/46339538/dplyrcount-multiple-columns)
#Increased intake since 2020
increased_1=factor(`Q33-1`),
increased_2=factor(`Q33-2`),
increased_3=factor(`Q33-3`),
decreased_1=factor(`Q34-1`),
decreased_2=factor(`Q34-2`),
decreased_3=factor(`Q34-3`),
#Behaviors
#Smoking status, and yes/no
smoking=factor(Q35),
smoking_c=factor(ifelse(Q35==3,0,1)),#variable 'smoking' as dichotomous categorical (yes=1/no=0)
shs_c=ifelse(Q36==2,0,1), #SHS exposure (yes=1/no=0)
#variable 'alcohol' as 3-levels, and create dichotomous categorical (yes=1/no=0)
alcohol=factor(Q37),
alcohol_c=factor(ifelse(Q37==3,0,1)),
#variable 'exercise' for categorical exercise habit (1 as highest)
exercise=factor(Q38),
#Family garden (yes=1/no=0) and gov garden (1 as highest, defined as '1' in gov_garden_c)
gov_garden=factor(Q39),
gov_garden_c=factor(ifelse(Q39==1,1,0)),
fam_garden=factor(ifelse(Q40==2,0,1)),
#COVID questionaire
covid_1=as.integer(Q41),
covid_2=as.integer(Q42),
covid_3=as.integer(Q43),
covid_4=as.integer(Q44),
covid_5=as.integer(Q45),
covid_6=as.integer(Q46),
covid_7=as.integer(Q47)
)
#Get food item names
food_names <- names(tuvalu1[c(38:62)])
#Select adult only, and this is the dataset we are going to use in the following analysis (2022)
tuvalu2<-tuvalu1%>%filter(highschool==2)
#================= Dietary pattern analysis ===================================
tuv_diet <- tuvalu2 %>% dplyr::select(c(`Participant No.`, all_of(food_names)))
names(tuv_diet) <- c("Participant", "Rice", "Taro", "Breadfruit", "Fish", "Pork", "Cabbage",
"Bird_nest_fern", "Banana", "Coconut", "Imp_fruits", "Eggs",
"Sweetened_bevs", "Ice_cream", "Potatoes", "Cassava",
"Instant_noodles", "Chicken", "Lamb_beef", "Cucumber",
"Imp_vegs", "Papaya", "Pandanus", "Milk",
"Chips_biscuits", "Cake")
tuv_diet_compl <- tuv_diet %>% drop_na() # complete cases only
diet_na_inds <- setdiff(tuv_diet$Participant, tuv_diet_compl$Participant) # dropped id's due to NAs
tuv_diet_compl <- tuv_diet_compl %>% dplyr::select(!(Participant))
write.csv(tuv_diet_compl, "diet_pattern_data.csv")
#============================ proceeding with 4 patterns =======================
set.seed(3)
lc4<-poLCA(f, data=tuv_diet_compl, nclass=4, na.rm = FALSE, nrep=30,
maxiter=3000, verbose=FALSE)
# formula for basic LCA
f <- as.formula(paste0("cbind(", paste0(names(tuv_diet_compl), collapse=", "), ")~1") )
lc4<-poLCA(f, data=tuv_diet_compl, nclass=4, na.rm = FALSE, nrep=30,
maxiter=3000, verbose=FALSE)
names_by_local <- c("Cassava", "Taro", "Breadfruit", "Cabbage", "Bird_nest_fern",
"Banana", "Coconut", "Papaya", "Pandanus", "Cucumber", "Fish",
"Rice", "Potatoes", "Imp_fruits", "Imp_vegs", "Chicken",
"Lamb_beef", "Eggs", "Milk", "Pork", "Sweetened_bevs",
"Ice_cream", "Instant_noodles", "Chips_biscuits", "Cake")
names_by_group <- c("Cassava", "Taro", "Rice", "Potatoes",
"Breadfruit", "Cabbage", "Bird_nest_fern", "Banana", "Coconut",
"Papaya", "Pandanus", "Cucumber", "Imp_fruits", "Imp_vegs",
"Fish", "Chicken", "Lamb_beef", "Pork",
"Eggs", "Milk",
"Sweetened_bevs", "Ice_cream", "Instant_noodles",
"Chips_biscuits", "Cake")
#================= Examine variables ===========================================
tuvalu4 <- tuvalu2[!(tuvalu2$`Participant No.` %in% diet_na_inds), ]
indiv_class <- lc4$predclass
tuvalu4$latent_class <- factor(indiv_class, levels=c(2, 1, 3, 4))
tuvalu4$obesity_1 <- factor(tuvalu4$obesity_1, levels=c(0,1))
tuvalu4$obesity_3 <- factor(tuvalu4$obesity_3, levels=c(0,1))
tuvalu4$age_center <- tuvalu4$age - mean(tuvalu4$age, na.rm = TRUE)
tuvalu4$Pattern <- factor(indiv_class, levels = c(1,2,3,4))
tuvalu4$height_center <- tuvalu4$`Height (cm)` - mean(tuvalu4$`Height (cm)`,
na.rm = TRUE)
write.csv(tuvalu4, "diet_pattern_data_analysis.csv")
get_output <- function(fit, exponentiate = TRUE) {
if (exponentiate) {
output <- data.frame(exp(fit$coefficients),
exp(posterior_interval(fit, prob = 0.95)[1:length(fit$coefficients),]))
colnames(output) <- c("Cond'l OR", "2.5%", "97.5%")
} else {
output <- data.frame(fit$coefficients,
posterior_interval(fit, prob = 0.95)[1:length(fit$coefficients),])
colnames(output) <- c("Mean", "2.5%", "97.5%")
}
print(paste0("Number of observations: ", nobs(fit)))
print(output)
}
# with interaction
set.seed(112)
fit_ob1_int <- stan_glmer(obesity_1 ~ latent_class + gender + age_center + education_c +
smoking_c + exercise + ncd + latent_class:gender +
latent_class:ncd + (1|region_c), data = tuvalu4,
family = binomial, adapt_delta = 0.999)
set.seed(122)
fit_ob3_int <- stan_glmer(obesity_3 ~ latent_class + gender + age_center + education_c +
smoking_c + exercise + ncd + latent_class:gender +
latent_class:ncd + (1|region_c), data = tuvalu4,
family = binomial, adapt_delta = 0.999)
set.seed(132)
fit_wt_int <- stan_glmer(`Weight (kg)` ~ latent_class + gender + age_center + education_c +
smoking_c + exercise + ncd + height_center + latent_class:gender +
latent_class:ncd + (1|region_c),
data = tuvalu4, adapt_delta = 0.999)
set.seed(111)
# OBESITY
fit_ob1 <- stan_glmer(obesity_1 ~ latent_class + gender + age_center + education_c +
smoking_c + exercise + ncd + (1|region_c), data = tuvalu4,
family = binomial, adapt_delta = 0.999)
# MORBID OBESITY
set.seed(121)
fit_ob3 <- stan_glmer(obesity_3 ~ latent_class + gender + age_center + education_c +
smoking_c + exercise + ncd + (1|region_c), data = tuvalu4,
family = binomial, adapt_delta = 0.999)
# WEIGHT (adding height as a variable)
set.seed(131)
fit_wt <- stan_glmer(`Weight (kg)` ~ latent_class + gender + age_center + education_c +
smoking_c + exercise + ncd + height_center + (1|region_c),
data = tuvalu4, adapt_delta = 0.999)
save(fit_ob1, fit_ob1_int, fit_ob3, fit_ob3_int, fit_wt, fit_wt_int,
file = "all_models.RData")
get_output(fit_ob1_int)
get_output(fit_ob3_int)
get_output(fit_ob3)
get_output(fit_wt)
get_output(fit_wt_int)
get_output(fit_wt, exponentiate = FALSE)
get_output(fit_wt_int, exponentiate = FALSE)
install.packages("epade")
library(epade)
means_int_wt <- tapply(tuvalu4$`Weight (kg)`, INDEX=list(tuvalu4$latent_class,
tuvalu4$ncd),
FUN = mean)
bar3d.ade(means_int_wt)
bar3d.ade(means_int_wt, col = "grey")
bar3d.ade(means_int_wt)
tuvalu4$`Weight (kg)`[tuvalu4$ncd == 0 & tuvalu4$latent_class == 3]
means_int_wt
means_int_wt <- tapply(tuvalu4$`Weight (kg)`, INDEX=list(tuvalu4$latent_class,
tuvalu4$ncd),
FUN = mean(na.rm = TRUE))
means_int_wt <- tapply(tuvalu4$`Weight (kg)`, INDEX=list(tuvalu4$latent_class,
tuvalu4$ncd),
function(x) = mean(x, na.rm = TRUE))
means_int_wt <- tapply(tuvalu4$`Weight (kg)`, INDEX=list(tuvalu4$latent_class,
tuvalu4$ncd),
function(x) {mean(x, na.rm = TRUE)})
bar3d.ade(means_int_wt)
summary(tuvalu4$age)
summary(tuvalu4$age_cat)
summary(tuvalu4$age_c)
